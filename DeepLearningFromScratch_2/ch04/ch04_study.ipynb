{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chapter 04. word2vec speed improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CBOW model is 2 layered model. it is simple, but takes longer time to process.  \n",
    "in this chapter we aim to add 2 improvements.  \n",
    "1. adding new layer called \"Embedding\"\n",
    "2. by \"Negative Sampling\", we use new loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 word2vec improvement(1)\n",
    "input is one-hot vector. suppose there are 1M vocabulary. then single vector has 1M binary digits.  \n",
    "=> solved by \"Embedding\"  \n",
    "hidden layer * output layer is too much caculation  \n",
    "=> solved by \"Negative Sampling\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Embedding layer\n",
    "one hot vector matrix multiplication is just extracting a row of matrix.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Embedding layer implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "W = np.arange(21).reshape(7,3)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.array([1, 0, 3, 0 ])\n",
    "W[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common/layers.py\n",
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "        \n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx]\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 word2vec improvement(2)\n",
    "by \"negative sampling\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 problem with caculation after hidden layer\n",
    "softmax is terrible when there are 1M variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 multi-class classification to binary classification\n",
    "we must find a way to make question that can be answered y/n.  \n",
    "example: \"if context is 'you' and 'goofbye', is target word 'say'?\"  \n",
    "this needs only one output.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Sigmoid, Cross Entropy Error.\n",
    "in CEE, if t=1, CEE(x) = -logy.  \n",
    "if t = 0, -log(1-y)  \n",
    "backpropagation of Sigmoid and CEE is exactly y-t! (163p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cho4/negative_sampling_layer.py\n",
    "\n",
    "class EmbeddingDot:\n",
    "    def __init__(self, W):\n",
    "        self.embed = Embedding(W)\n",
    "        self.params = self.embed.params\n",
    "        self.grads = self.embed.grads\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, h, idx):\n",
    "        target_W  = self.embed.forward(idx)\n",
    "        out = np.sum(target_W * h, axis = 1)\n",
    "        \n",
    "        self.cache = (h, target_W)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        h, target_W = self.cache\n",
    "        dout = dout.reshape(dout.reshape[0], 1) # Q: ?\n",
    "        dtarget_W = dout * h\n",
    "        self.embed.backward(dtarget_W)\n",
    "        dh = dout * target_W\n",
    "        return dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in here idx is np.array of word. its array because it wants to process minibatch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "W = np.array([[0, 1, 2],\n",
    "              [3, 4, 5],\n",
    "              [6, 7, 8],\n",
    "              [9, 10, 11],\n",
    "              [12, 13, 14], \n",
    "              [15, 16, 17],\n",
    "              [18, 19, 20]])\n",
    "\n",
    "idx = np.array([0, 3, 1])\n",
    "\n",
    "h = np.array([[0, 1, 2],\n",
    "            [3, 4, 5],\n",
    "            [6, 7, 8]])\n",
    "\n",
    "embed = Embedding(W)\n",
    "print(embed)\n",
    "\n",
    "target_W = embed.forward(idx)\n",
    "print(target_W)\n",
    "\n",
    "out = np.sum(target_W * h, axis = 1)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.5 Negative Sampling\n",
    "ok, so we now can train model for correct target.  \n",
    "but what happens if target words are wrong?  \n",
    "we dont want to train binary classification for EVERY wrong words. for it takes too much time.  \n",
    "thus, we SELECT few wrong cases. and that is Negative Sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.6 Sampling method \n",
    "we have better method than random.  \n",
    "we can get statistic of corpus, and select frequent words from it.  \n",
    "np.random.choice()method allows this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.choice(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['you', 'say', 'goodbye', 'I', 'hello', '.']\n",
    "np.random.choice(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(words, size = 5) # 5 sample, overlap OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(words, size = 5, replace = False) # 5 sample, overlap NO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [0.5, 0.1, 0.05, 0.2, 0.05, 0.1]\n",
    "np.random.choice(words, p = p) # we can set probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "its all good. but word2vec suggests to power 0.75 for probabilities.  \n",
    "P(w_i) ** 0.75.  \n",
    "to consider less probable words.  \n",
    "we implemented this function in UnigramSampler.  \n",
    "check ch04/negative_sampling_layer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.7 Negative Sampling implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from negative_sampling_layer import UnigramSampler\n",
    "from common.layers import Embedding, SigmoidWithLoss\n",
    "\n",
    "# ch04/negative_sampling_layer.py\n",
    "class NegativeSamplingLoss:\n",
    "    def __init__(self, W, corpus, power = 0.75, sample_size = 5):\n",
    "        self.sample_size = sample_size\n",
    "        self.sampler = UnigramSampler(corpus, power, sample_size)\n",
    "        self.loss_layer = [SigmoidWithLoss() for _ in range(sample_size + 1)]\n",
    "        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)]\n",
    "        \n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.embed_dot_layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "            \n",
    "    def forward(self, h, target):\n",
    "        batch_size = target.shape[0]\n",
    "        negative_sample = self.sampler.get_negative_sample(target)\n",
    "        \n",
    "        #positive case forward\n",
    "        score = self.embed_dot_layers[0].forward(h, target)\n",
    "        correct_label = np.ones(batch_size, dtype=np.int32)\n",
    "        loss = self.loss_layer[0].forward(score, correct_label)\n",
    "        \n",
    "        #negative case forward\n",
    "        negative_label = np.zeros(batch_size, dtype=np.int32)\n",
    "        for i in range(self.sample_size):\n",
    "            negative_target = negative_sample[:, i]\n",
    "            score = self.embed_dot_layers[1 + i].forward(h, negative_target)\n",
    "            loss += self.loss_layer[1 + i].forward(score, negative_label)\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout = 1):\n",
    "        dh = 0\n",
    "        for l0, l1 in zip(self.loss_layer, self.embed_dot_layers):\n",
    "            dscore = l0.backward(dout)\n",
    "            dh += l1.backward(dscore)\n",
    "            \n",
    "        return dh\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 improved word2vec training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 CBOW model implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "its in ch04/cbow.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.np import *  # import numpy as np\n",
    "from common.layers import Embedding\n",
    "from ch04.negative_sampling_layer import NegativeSamplingLoss\n",
    "\n",
    "class CBOW:\n",
    "    def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
    "        V, H = vocab_size, hidden_size\n",
    "\n",
    "        # 가중치 초기화\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(V, H).astype('f')\n",
    "\n",
    "        # 계층 생성\n",
    "        self.in_layers = []\n",
    "        for i in range(2 * window_size):\n",
    "            layer = Embedding(W_in)  # Embedding 계층 사용\n",
    "            self.in_layers.append(layer)\n",
    "        self.ns_loss = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)\n",
    "\n",
    "        # 모든 가중치와 기울기를 배열에 모은다.\n",
    "        layers = self.in_layers + [self.ns_loss]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        # 인스턴스 변수에 단어의 분산 표현을 저장한다.\n",
    "        self.word_vecs = W_in\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        h = 0\n",
    "        for i, layer in enumerate(self.in_layers):\n",
    "            h += layer.forward(contexts[:, i])\n",
    "        h *= 1 / len(self.in_layers)\n",
    "        loss = self.ns_loss.forward(h, target)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.ns_loss.backward(dout)\n",
    "        dout *= 1 / len(self.in_layers)\n",
    "        for layer in self.in_layers:\n",
    "            layer.backward(dout)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 CBOW training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ch04 import cbow, train, eval\n",
    "# all this training takes... half a day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
